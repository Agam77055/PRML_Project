{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejM1JQp8xaMf"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkNgoSUwrIlU",
        "outputId": "295a2724-c87b-472f-b93c-32a4b54a4454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wn in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from wn) (0.28.1)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.11/dist-packages (from wn) (2.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->wn) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->wn) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->wn) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->wn) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->wn) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->wn) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->wn) (4.13.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install wn\n",
        "\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import wn\n",
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrlQXOKOxkr4"
      },
      "source": [
        "# Downloading Pre-Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pbcOucYxq9R",
        "outputId": "79bd09b2-657b-483a-90f4-431238ac7d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[KCached file found: /root/.wn_data/downloads/3334cfd8709f5032fe246261d73528528c2542fa\n",
            "\u001b[KSkipping omw-en:1.4 (OMW English Wordnet based on WordNet 3.0); already added\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download WordNet data\n",
        "try:\n",
        "    wn.download(\"omw-en\")\n",
        "except:\n",
        "    print(\"WordNet data already downloaded or couldn't be downloaded.\")\n",
        "\n",
        "# Load contractions dictionary\n",
        "try:\n",
        "    with open(\"contractions_dict.json\", \"r\") as contractions:\n",
        "        contractions_dict = json.load(contractions)\n",
        "except FileNotFoundError:\n",
        "    print(\"Contractions dictionary not found. Creating a basic one.\")\n",
        "    contractions_dict = {\n",
        "        \"can't\": \"cannot\", \"won't\": \"will not\", \"don't\": \"do not\",\n",
        "        \"doesn't\": \"does not\", \"i'm\": \"i am\", \"you're\": \"you are\",\n",
        "        \"we're\": \"we are\", \"they're\": \"they are\", \"it's\": \"it is\",\n",
        "        \"he's\": \"he is\", \"she's\": \"she is\", \"that's\": \"that is\",\n",
        "        \"what's\": \"what is\", \"where's\": \"where is\", \"when's\": \"when is\",\n",
        "        \"why's\": \"why is\", \"how's\": \"how is\", \"i've\": \"i have\",\n",
        "        \"you've\": \"you have\", \"we've\": \"we have\", \"they've\": \"they have\",\n",
        "        \"i'd\": \"i would\", \"you'd\": \"you would\", \"he'd\": \"he would\",\n",
        "        \"she'd\": \"she would\", \"we'd\": \"we would\", \"they'd\": \"they would\",\n",
        "        \"i'll\": \"i will\", \"you'll\": \"you will\", \"he'll\": \"he will\",\n",
        "        \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\"\n",
        "    }\n",
        "    with open(\"contractions_dict.json\", \"w\") as f:\n",
        "        json.dump(contractions_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVe_5aSuxxSJ"
      },
      "source": [
        "# Pre Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aOQSCgQdx5f_"
      },
      "outputs": [],
      "source": [
        "class PreProcessing:\n",
        "    \"\"\"Text preprocessing module with advanced tokenization\"\"\"\n",
        "\n",
        "    class Tokenizer:\n",
        "        \"\"\"Advanced tokenizer with MWE recognition and contraction handling\"\"\"\n",
        "\n",
        "        def __init__(self):\n",
        "            try:\n",
        "                self.wordnet = wn.Wordnet(\"omw-en\")\n",
        "                self.MWEs = self.list_MWEs()\n",
        "            except:\n",
        "                print(\"WordNet initialization failed. Using basic tokenization.\")\n",
        "                self.MWEs = []\n",
        "            self.compile_regex_patterns()\n",
        "\n",
        "        def list_MWEs(self):\n",
        "            \"\"\"Extract multi-word expressions (MWEs) from WordNet.\"\"\"\n",
        "            MWEs = []\n",
        "\n",
        "            try:\n",
        "                # Get multi-word nouns\n",
        "                nouns = self.wordnet.synsets(pos=\"n\")\n",
        "                MWEs.extend([syn.lemmas()[0] for syn in nouns if \" \" in syn.lemmas()[0]])\n",
        "\n",
        "                # Get multi-word verbs\n",
        "                verbs = self.wordnet.synsets(pos=\"v\")\n",
        "                MWEs.extend([syn.lemmas()[0] for syn in verbs if \" \" in syn.lemmas()[0]])\n",
        "            except:\n",
        "                print(\"Error extracting MWEs. Using empty list.\")\n",
        "\n",
        "            return MWEs\n",
        "\n",
        "        def compile_regex_patterns(self):\n",
        "            \"\"\"Compile all required regex patterns.\"\"\"\n",
        "            # Multi-word expressions handling\n",
        "            if self.MWEs:\n",
        "                mwe_patterns = [rf\"\\b{re.escape(mwe)}\\b\" for mwe in self.MWEs]\n",
        "                self.regex_pattern = re.compile(\"|\".join(mwe_patterns))\n",
        "            else:\n",
        "                self.regex_pattern = re.compile(r\"\")\n",
        "\n",
        "            # Hyphen handling (e.g., \"high-quality\" → \"high quality\")\n",
        "            self.hyphen_pattern = re.compile(r\"\\b(\\w+)-(\\w+)\\b\")\n",
        "\n",
        "            # Preserve numbers with units (e.g., \"10kg\" → \"10_kg\", \"$100\" → \"$100\")\n",
        "            self.number_unit_pattern = re.compile(r\"(\\d+)([a-zA-Z]+)\")\n",
        "\n",
        "            # Punctuation removal (except in preserved cases)\n",
        "            self.punctuation_pattern = re.compile(r\"[^\\w\\s\\-_]\")\n",
        "\n",
        "            # Contractions patterns\n",
        "            self.contraction_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, contractions_dict.keys())) + r\")\\b\", re.IGNORECASE)\n",
        "\n",
        "        def tokenize(self, text):\n",
        "            \"\"\"Tokenize the input text with preprocessing steps.\"\"\"\n",
        "            # Convert to lowercase\n",
        "            text = text.lower()\n",
        "\n",
        "            # Replace multi-word expressions with underscores\n",
        "            if self.MWEs:\n",
        "                text = self.regex_pattern.sub(lambda match: match.group(0).replace(\" \", \"_\"), text)\n",
        "\n",
        "            # Handle contractions\n",
        "            text = self.contraction_pattern.sub(lambda match: contractions_dict.get(match.group(0).lower(), match.group(0)), text)\n",
        "\n",
        "            # Handle hyphens (convert to spaces)\n",
        "            text = self.hyphen_pattern.sub(r\"\\1 \\2\", text)\n",
        "\n",
        "            # Preserve numbers with units\n",
        "            text = self.number_unit_pattern.sub(r\"\\1_\\2\", text)\n",
        "\n",
        "            # Remove other punctuation\n",
        "            text = self.punctuation_pattern.sub(\"\", text)\n",
        "\n",
        "            # Tokenize by splitting on whitespace\n",
        "            tokens = text.split()\n",
        "            return tokens\n",
        "\n",
        "    class tf_idf_Vectorizer:\n",
        "        \"\"\"TF-IDF Vectorizer implementation with advanced features\"\"\"\n",
        "\n",
        "        def __init__(self, max_features=None):\n",
        "            self.vocabulary = {}\n",
        "            self.idf = {}\n",
        "            self.max_features = max_features\n",
        "            self.fitted = False\n",
        "\n",
        "        def fit(self, corpus):\n",
        "            \"\"\"Build vocabulary with unique indices and compute IDF values.\"\"\"\n",
        "            if isinstance(corpus[0], str):\n",
        "                # If corpus is a list of strings, tokenize them\n",
        "                tokenizer = PreProcessing.Tokenizer()\n",
        "                corpus = [tokenizer.tokenize(doc) for doc in corpus]\n",
        "            elif not isinstance(corpus[0], list):\n",
        "                raise ValueError(\"Corpus must be a list of strings or tokenized documents (list of lists).\")\n",
        "\n",
        "            # Count document frequency (DF) for each word\n",
        "            df = Counter()\n",
        "            for doc in corpus:\n",
        "                unique_words = set(doc)\n",
        "                df.update(unique_words)\n",
        "\n",
        "            # Sort words by DF in descending order\n",
        "            sorted_words = [word for word, _ in df.most_common(self.max_features)] if self.max_features else list(df.keys())\n",
        "\n",
        "            # Create vocabulary\n",
        "            self.vocabulary = {word: idx for idx, word in enumerate(sorted_words)}\n",
        "\n",
        "            # Compute IDF with smoothing: log((N + 1) / (df + 1)) + 1\n",
        "            N = len(corpus)\n",
        "            self.idf = {word: np.log((N + 1) / (df[word] + 1)) + 1 for word in self.vocabulary}\n",
        "\n",
        "            self.fitted = True\n",
        "            return self\n",
        "\n",
        "        def transform(self, documents):\n",
        "            \"\"\"Convert new documents into TF-IDF vectors using learned vocabulary.\"\"\"\n",
        "            if not self.fitted:\n",
        "                raise ValueError(\"Vectorizer needs to be fitted before transform\")\n",
        "\n",
        "            if isinstance(documents[0], str):\n",
        "                # If documents is a list of strings, tokenize them\n",
        "                tokenizer = PreProcessing.Tokenizer()\n",
        "                documents = [tokenizer.tokenize(doc) for doc in documents]\n",
        "            elif not isinstance(documents[0], list):\n",
        "                raise ValueError(\"Input documents must be strings or tokenized documents (list of lists).\")\n",
        "\n",
        "            tfidf_matrix = np.zeros((len(documents), len(self.vocabulary)))\n",
        "\n",
        "            for i, doc in enumerate(documents):\n",
        "                # Term frequency for the document\n",
        "                tf = Counter(doc)\n",
        "                total_words = len(doc)\n",
        "\n",
        "                for word, count in tf.items():\n",
        "                    if word in self.vocabulary:  # Ignore unseen words\n",
        "                        word_idx = self.vocabulary[word]\n",
        "                        tfidf_matrix[i][word_idx] = (count / total_words) * self.idf.get(word, 0)\n",
        "\n",
        "            return tfidf_matrix\n",
        "\n",
        "        def fit_transform(self, documents):\n",
        "            \"\"\"Fit and transform documents\"\"\"\n",
        "            self.fit(documents)\n",
        "            return self.transform(documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vh90MH-x8YE"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2iuMtSIEyEwV"
      },
      "outputs": [],
      "source": [
        "class SVMFromScratch:\n",
        "    \"\"\"Support Vector Machine classifier implemented from scratch\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.classes = None\n",
        "\n",
        "    def _init_weights_bias(self, X):\n",
        "        \"\"\"Initialize weights and bias\"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit SVM using gradient descent\"\"\"\n",
        "        # Save original class labels\n",
        "        self.classes = np.unique(y)\n",
        "\n",
        "        # If binary classification, convert to {-1, 1}\n",
        "        if len(self.classes) == 2:\n",
        "            y_binary = np.where(y == self.classes[0], -1, 1)\n",
        "        else:\n",
        "            raise ValueError(\"This SVM implementation only supports binary classification\")\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self._init_weights_bias(X)\n",
        "\n",
        "        # Gradient descent\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Calculate hinge loss condition\n",
        "                condition = y_binary[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "\n",
        "                # Update weights\n",
        "                if condition:\n",
        "                    self.w = self.w - self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    self.w = self.w - self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_binary[idx]))\n",
        "                    self.b = self.b - self.lr * y_binary[idx]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_raw(self, X):\n",
        "        \"\"\"Predict raw values\"\"\"\n",
        "        return np.dot(X, self.w) - self.b\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels\"\"\"\n",
        "        raw_preds = self.predict_raw(X)\n",
        "        y_pred = np.where(raw_preds <= 0, self.classes[0], self.classes[1])\n",
        "        return y_pred\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities using sigmoid function\"\"\"\n",
        "        raw_preds = self.predict_raw(X)\n",
        "        # Apply sigmoid function\n",
        "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "        probs = sigmoid(raw_preds)\n",
        "\n",
        "        # Return probabilities for both classes\n",
        "        return np.column_stack((1 - probs, probs))\n",
        "\n",
        "\n",
        "class NaiveBayesFromScratch:\n",
        "    \"\"\"Multinomial Naive Bayes classifier implemented from scratch\"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha  # Smoothing parameter\n",
        "        self.class_priors = {}\n",
        "        self.feature_probs = {}\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit Naive Bayes model\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Calculate class priors\n",
        "        for c in self.classes:\n",
        "            self.class_priors[c] = np.sum(y == c) / n_samples\n",
        "\n",
        "        # Calculate feature probabilities for each class\n",
        "        for c in self.classes:\n",
        "            # Get samples for this class\n",
        "            X_c = X[y == c]\n",
        "\n",
        "            # Calculate feature probabilities with Laplace smoothing\n",
        "            # Sum of each feature for samples in this class\n",
        "            feature_counts = np.sum(X_c, axis=0) + self.alpha\n",
        "            # Total counts for this class (with smoothing)\n",
        "            total_counts = np.sum(feature_counts)\n",
        "\n",
        "            # Store probabilities\n",
        "            self.feature_probs[c] = feature_counts / total_counts\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Initialize probability matrix\n",
        "        probs = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        # Calculate log probabilities for each class\n",
        "        for i, c in enumerate(self.classes):\n",
        "            # Class prior (in log space)\n",
        "            class_prior = np.log(self.class_priors[c])\n",
        "            # Feature probabilities (in log space)\n",
        "            feature_probs = np.log(self.feature_probs[c])\n",
        "\n",
        "            # Calculate log probability for each sample\n",
        "            for j, x in enumerate(X):\n",
        "                # Only consider non-zero features (TF-IDF scores)\n",
        "                non_zero_indices = x > 0\n",
        "                if np.any(non_zero_indices):\n",
        "                    # Multiply feature values by log probabilities\n",
        "                    log_prob = class_prior + np.sum(x[non_zero_indices] * feature_probs[non_zero_indices])\n",
        "                else:\n",
        "                    log_prob = class_prior\n",
        "\n",
        "                probs[j, i] = log_prob\n",
        "\n",
        "        # Convert log probabilities to probabilities\n",
        "        # Subtract max for numerical stability\n",
        "        probs_exp = np.exp(probs - np.max(probs, axis=1, keepdims=True))\n",
        "        probs_normalized = probs_exp / np.sum(probs_exp, axis=1, keepdims=True)\n",
        "\n",
        "        return probs_normalized\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels\"\"\"\n",
        "        probs = self.predict_proba(X)\n",
        "        return self.classes[np.argmax(probs, axis=1)]\n",
        "\n",
        "\n",
        "class DecisionTreeFromScratch:\n",
        "    \"\"\"Decision Tree classifier implemented from scratch\"\"\"\n",
        "\n",
        "    class Node:\n",
        "        \"\"\"Node in decision tree\"\"\"\n",
        "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "            self.feature = feature      # Index of feature to split on\n",
        "            self.threshold = threshold  # Threshold value for split\n",
        "            self.left = left            # Left subtree (True branch)\n",
        "            self.right = right          # Right subtree (False branch)\n",
        "            self.value = value          # Class label (for leaf nodes)\n",
        "\n",
        "    def __init__(self, max_depth=10, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build decision tree\"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "        return self\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively grow tree\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (depth >= self.max_depth or\n",
        "            n_samples < self.min_samples_split or\n",
        "            n_classes == 1):\n",
        "            # Create leaf node\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return self.Node(value=leaf_value)\n",
        "\n",
        "        # Find best split\n",
        "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
        "\n",
        "        # Create subtrees\n",
        "        left_idxs = X[:, best_feature] < best_threshold\n",
        "        right_idxs = ~left_idxs\n",
        "\n",
        "        # Recursively build subtrees\n",
        "        left_subtree = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
        "\n",
        "        return self.Node(best_feature, best_threshold, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y, n_features):\n",
        "        \"\"\"Find best feature and threshold for split\"\"\"\n",
        "        best_gain = -float('inf')\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Try a limited number of features (for efficiency)\n",
        "        features_to_try = random.sample(range(n_features), min(n_features, 20))\n",
        "\n",
        "        for feature in features_to_try:\n",
        "            # Get unique values for this feature\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "\n",
        "            # Try different thresholds\n",
        "            for threshold in thresholds:\n",
        "                # Calculate information gain\n",
        "                gain = self._information_gain(y, X[:, feature], threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _information_gain(self, y, feature_values, threshold):\n",
        "        \"\"\"Calculate information gain for a split\"\"\"\n",
        "        # Calculate parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # Create children\n",
        "        left_idxs = feature_values < threshold\n",
        "        right_idxs = ~left_idxs\n",
        "\n",
        "        # If split creates empty node, return no gain\n",
        "        if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # Calculate weighted entropy of children\n",
        "        n = len(y)\n",
        "        n_left, n_right = np.sum(left_idxs), np.sum(right_idxs)\n",
        "\n",
        "        entropy_left = self._entropy(y[left_idxs])\n",
        "        entropy_right = self._entropy(y[right_idxs])\n",
        "\n",
        "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
        "\n",
        "        # Calculate information gain\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"Calculate entropy of a set\"\"\"\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        ps = ps[ps > 0]  # Remove zeros\n",
        "        return -np.sum(ps * np.log2(ps))\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        \"\"\"Find most common label in a set\"\"\"\n",
        "        counter = Counter(y)\n",
        "        return counter.most_common(1)[0][0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels\"\"\"\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Traverse tree to make prediction\"\"\"\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] < node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(x, node.right)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        probas = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for i, x in enumerate(X):\n",
        "            # Traverse tree to find leaf node\n",
        "            leaf_value = self._traverse_tree(x, self.root)\n",
        "            # Set probability to 1.0 for predicted class\n",
        "            class_idx = np.where(self.classes == leaf_value)[0][0]\n",
        "            probas[i, class_idx] = 1.0\n",
        "\n",
        "        return probas\n",
        "\n",
        "\n",
        "class RandomForestFromScratch:\n",
        "    \"\"\"Random Forest classifier implemented from scratch\"\"\"\n",
        "\n",
        "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, sample_ratio=0.8):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.sample_ratio = sample_ratio\n",
        "        self.trees = []\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build random forest\"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "\n",
        "        # Create bootstrap samples and train trees\n",
        "        for _ in range(self.n_trees):\n",
        "            # Bootstrap sampling\n",
        "            n_samples = X.shape[0]\n",
        "            sample_size = int(n_samples * self.sample_ratio)\n",
        "\n",
        "            # Sample with replacement\n",
        "            idxs = np.random.choice(n_samples, size=sample_size, replace=True)\n",
        "            X_sample, y_sample = X[idxs], y[idxs]\n",
        "\n",
        "            # Train decision tree\n",
        "            tree = DecisionTreeFromScratch(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split\n",
        "            )\n",
        "            tree.fit(X_sample, y_sample)\n",
        "\n",
        "            # Add tree to forest\n",
        "            self.trees.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels by majority vote\"\"\"\n",
        "        # Get predictions from all trees\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "\n",
        "        # Transpose to get one row per sample, with columns as tree predictions\n",
        "        tree_preds = tree_preds.T\n",
        "\n",
        "        # Apply majority vote for each sample\n",
        "        final_preds = np.array([self._majority_vote(pred) for pred in tree_preds])\n",
        "\n",
        "        return final_preds\n",
        "\n",
        "    def _majority_vote(self, predictions):\n",
        "        \"\"\"Apply majority voting\"\"\"\n",
        "        counter = Counter(predictions)\n",
        "        return counter.most_common(1)[0][0]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Initialize probabilities\n",
        "        probas = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        # Get predictions from all trees\n",
        "        for tree in self.trees:\n",
        "            tree_proba = tree.predict_proba(X)\n",
        "            probas += tree_proba\n",
        "\n",
        "        # Average probabilities\n",
        "        probas /= self.n_trees\n",
        "\n",
        "        return probas\n",
        "\n",
        "\n",
        "def cosine_similarity_from_scratch(A, B):\n",
        "    \"\"\"Calculate cosine similarity between matrices\"\"\"\n",
        "    # Compute dot product\n",
        "    dot_product = np.dot(A, B.T)\n",
        "\n",
        "    # Compute norms\n",
        "    norm_A = np.sqrt(np.sum(A**2, axis=1, keepdims=True))\n",
        "    norm_B = np.sqrt(np.sum(B**2, axis=1, keepdims=True))\n",
        "\n",
        "    # Avoid division by zero\n",
        "    norm_A[norm_A == 0] = 1e-10\n",
        "    norm_B[norm_B == 0] = 1e-10\n",
        "\n",
        "    # Compute similarity\n",
        "    similarity = dot_product / (norm_A * norm_B.T)\n",
        "\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpD2ocIiyGU9"
      },
      "source": [
        "# Intent Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6_Ex_eqNyLeQ"
      },
      "outputs": [],
      "source": [
        "class EnhancedIntentClassifier:\n",
        "    \"\"\"Intent classification with advanced preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, model_type='naive_bayes', max_features=1000):\n",
        "        self.model_type = model_type\n",
        "        self.tokenizer = PreProcessing.Tokenizer()\n",
        "        self.vectorizer = PreProcessing.tf_idf_Vectorizer(max_features=max_features)\n",
        "\n",
        "        # Choose model based on type\n",
        "        if model_type == 'svm':\n",
        "            self.model = SVMFromScratch(learning_rate=0.01, lambda_param=0.01, n_iters=1000)\n",
        "        elif model_type == 'naive_bayes':\n",
        "            self.model = NaiveBayesFromScratch(alpha=1.0)\n",
        "        elif model_type == 'random_forest':\n",
        "            self.model = RandomForestFromScratch(n_trees=10, max_depth=5)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "    def train(self, texts, labels, test_size=0.2):\n",
        "        \"\"\"Train the intent classifier\"\"\"\n",
        "        print(f\"Tokenizing {len(texts)} training examples...\")\n",
        "        # Tokenize texts\n",
        "        tokenized_texts = [self.tokenizer.tokenize(text) for text in texts]\n",
        "\n",
        "        print(\"Vectorizing text...\")\n",
        "        # Vectorize text\n",
        "        X_vec = self.vectorizer.fit(tokenized_texts)\n",
        "        X_vec = self.vectorizer.transform(tokenized_texts)\n",
        "\n",
        "        # Convert labels to numpy array if needed\n",
        "        y = np.array(labels)\n",
        "\n",
        "        # Split data for evaluation\n",
        "        n_samples = len(texts)\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_size = int((1 - test_size) * n_samples)\n",
        "        train_indices = indices[:train_size]\n",
        "        test_indices = indices[train_size:]\n",
        "\n",
        "        X_train = X_vec[train_indices]\n",
        "        y_train = y[train_indices]\n",
        "        X_test = X_vec[test_indices]\n",
        "        y_test = y[test_indices]\n",
        "\n",
        "        print(f\"Training {self.model_type} model...\")\n",
        "        # Train model\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        accuracy = np.mean(y_pred == y_test)\n",
        "        print(f\"Intent Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predict intent from text\"\"\"\n",
        "        # Tokenize and vectorize\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        X_vec = self.vectorizer.transform([tokens])\n",
        "\n",
        "        # Predict\n",
        "        intent = self.model.predict(X_vec)[0]\n",
        "\n",
        "        # Get probability\n",
        "        prob_matrix = self.model.predict_proba(X_vec)\n",
        "\n",
        "        # Find probability for predicted class\n",
        "        class_idx = np.where(self.model.classes == intent)[0][0]\n",
        "        prob = prob_matrix[0, class_idx]\n",
        "\n",
        "        return intent, prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAfoYGHByO0b"
      },
      "source": [
        "# Entity Exctractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0_8bZ5maySWf"
      },
      "outputs": [],
      "source": [
        "class EntityExtractorFromScratch:\n",
        "    \"\"\"Rule-based entity extraction\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.entity_patterns = {}\n",
        "\n",
        "    def add_entity_pattern(self, entity_name, pattern):\n",
        "        \"\"\"Add regex pattern for entity extraction\"\"\"\n",
        "        self.entity_patterns[entity_name] = re.compile(pattern, re.IGNORECASE)\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        \"\"\"Extract entities from text based on patterns\"\"\"\n",
        "        entities = {}\n",
        "        for entity_name, pattern in self.entity_patterns.items():\n",
        "            matches = pattern.findall(text)\n",
        "            if matches:\n",
        "                entities[entity_name] = matches\n",
        "        return entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jopzGXm_yWnu"
      },
      "source": [
        "# Response Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c_n068XtyZ30"
      },
      "outputs": [],
      "source": [
        "class ResponseGeneratorFromScratch:\n",
        "    \"\"\"Response generation using retrieval-based approach\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer = PreProcessing.Tokenizer()\n",
        "        self.vectorizer = PreProcessing.tf_idf_Vectorizer()\n",
        "        self.responses_df = None\n",
        "        self.response_vectors = None\n",
        "\n",
        "    def train(self, intents_responses_df):\n",
        "        \"\"\"Train response generator on intent-response pairs\"\"\"\n",
        "        self.responses_df = intents_responses_df\n",
        "\n",
        "        # Tokenize intent templates\n",
        "        tokenized_templates = [self.tokenizer.tokenize(template)\n",
        "                               for template in self.responses_df['intent_template'].tolist()]\n",
        "\n",
        "        # Vectorize intent templates\n",
        "        self.vectorizer.fit(tokenized_templates)\n",
        "        self.response_vectors = self.vectorizer.transform(tokenized_templates)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def generate_response(self, user_input, intent=None, entities=None, threshold=0.3):\n",
        "        \"\"\"Generate response based on user input\"\"\"\n",
        "        # If intent is provided, use it\n",
        "        if intent and intent in set(self.responses_df['intent']):\n",
        "            # Filter responses by intent\n",
        "            intent_responses = self.responses_df[self.responses_df['intent'] == intent]\n",
        "            # Select random response for this intent\n",
        "            response_idx = random.randint(0, len(intent_responses) - 1)\n",
        "            response = intent_responses.iloc[response_idx]['response']\n",
        "            return response, 1.0\n",
        "\n",
        "        # Otherwise, use retrieval-based approach\n",
        "        tokens = self.tokenizer.tokenize(user_input)\n",
        "        input_vector = self.vectorizer.transform([tokens])\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity_from_scratch(input_vector, self.response_vectors)\n",
        "\n",
        "        # Get best match\n",
        "        best_match_idx = np.argmax(similarities[0])\n",
        "        best_match_score = similarities[0][best_match_idx]\n",
        "\n",
        "        # Check if similarity is above threshold\n",
        "        if best_match_score >= threshold:\n",
        "            response = self.responses_df.iloc[best_match_idx]['response']\n",
        "            return response, best_match_score\n",
        "        else:\n",
        "            return \"I'm not sure how to respond to that.\", 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVboPyHcycQ6"
      },
      "source": [
        "# Dialogue Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JJk_9DwMyfnQ"
      },
      "outputs": [],
      "source": [
        "class DialogueManagerFromScratch:\n",
        "    \"\"\"Simple state-based dialogue manager\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.context = {}\n",
        "        self.current_state = \"greeting\"\n",
        "        self.state_transitions = {}\n",
        "\n",
        "    def add_state_transition(self, current_state, intent, next_state):\n",
        "        \"\"\"Define state transition based on intent\"\"\"\n",
        "        if current_state not in self.state_transitions:\n",
        "            self.state_transitions[current_state] = {}\n",
        "        self.state_transitions[current_state][intent] = next_state\n",
        "\n",
        "    def update_state(self, intent):\n",
        "        \"\"\"Update dialogue state based on intent\"\"\"\n",
        "        if self.current_state in self.state_transitions and intent in self.state_transitions[self.current_state]:\n",
        "            self.current_state = self.state_transitions[self.current_state][intent]\n",
        "        return self.current_state\n",
        "\n",
        "    def update_context(self, entities):\n",
        "        \"\"\"Update context with extracted entities\"\"\"\n",
        "        if entities:\n",
        "            for entity_type, values in entities.items():\n",
        "                self.context[entity_type] = values\n",
        "\n",
        "    def get_context(self):\n",
        "        \"\"\"Get current dialogue context\"\"\"\n",
        "        return self.context\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset dialogue manager\"\"\"\n",
        "        self.context = {}\n",
        "        self.current_state = \"greeting\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5YRnhwtyhX1"
      },
      "source": [
        "# Chat Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TAvAKMtllwIp"
      },
      "outputs": [],
      "source": [
        "class EnhancedChatbot:\n",
        "    \"\"\"Chatbot with advanced preprocessing and dialogue management - DailyDialog Dataset Support\"\"\"\n",
        "\n",
        "    def __init__(self, bot_name=\"EnhancedBot\", model_type):\n",
        "        self.name = bot_name\n",
        "        self.intent_classifier = EnhancedIntentClassifier(model_type=model_type)\n",
        "        self.entity_extractor = EntityExtractorFromScratch()\n",
        "        self.response_generator = ResponseGeneratorFromScratch()\n",
        "        self.dialogue_manager = DialogueManagerFromScratch()\n",
        "\n",
        "        # Topic mapping\n",
        "        self.topic_mapping = {\n",
        "            1: \"Ordinary Life\",\n",
        "            2: \"School Life\",\n",
        "            3: \"Culture & Education\",\n",
        "            4: \"Attitude & Emotion\",\n",
        "            5: \"Relationship\",\n",
        "            6: \"Tourism\",\n",
        "            7: \"Health\",\n",
        "            8: \"Work\",\n",
        "            9: \"Politics\",\n",
        "            10: \"Finance\"\n",
        "        }\n",
        "\n",
        "        # Dialog act mapping\n",
        "        self.act_mapping = {\n",
        "            1: \"inform\",\n",
        "            2: \"question\",\n",
        "            3: \"directive\",\n",
        "            4: \"commissive\"\n",
        "        }\n",
        "\n",
        "        # Emotion mapping\n",
        "        self.emotion_mapping = {\n",
        "            0: \"neutral\",\n",
        "            1: \"anger\",\n",
        "            2: \"disgust\",\n",
        "            3: \"fear\",\n",
        "            4: \"happiness\",\n",
        "            5: \"sadness\",\n",
        "            6: \"surprise\"\n",
        "        }\n",
        "\n",
        "    def train(self, dialogues_file=\"dialogues_text.txt\", topics_file=\"dialogues_topic.txt\",\n",
        "              acts_file=\"dialogues_act.txt\", emotions_file=\"dialogues_emotion.txt\", test_size=0.2):\n",
        "        \"\"\"Train the chatbot on DailyDialog dataset\"\"\"\n",
        "        print(f\"Loading DailyDialog dataset...\")\n",
        "\n",
        "        try:\n",
        "            # Load the dialogue files\n",
        "            with open(dialogues_file, 'r', encoding='utf-8') as f:\n",
        "                dialogues = f.readlines()\n",
        "\n",
        "            with open(topics_file, 'r', encoding='utf-8') as f:\n",
        "                topics = f.readlines()\n",
        "\n",
        "            with open(acts_file, 'r', encoding='utf-8') as f:\n",
        "                acts = f.readlines()\n",
        "\n",
        "            with open(emotions_file, 'r', encoding='utf-8') as f:\n",
        "                emotions = f.readlines()\n",
        "\n",
        "            # Process the data into structured format\n",
        "            processed_data = self._process_dailydialog_data(dialogues, topics, acts, emotions)\n",
        "\n",
        "            # Train intent classifier (using dialog acts as intents)\n",
        "            print(f\"Training intent classifier on {len(processed_data['utterances'])} examples\")\n",
        "            self.intent_classifier.train(\n",
        "                texts=processed_data['utterances'],\n",
        "                labels=processed_data['acts_labels'],\n",
        "                test_size=test_size\n",
        "            )\n",
        "\n",
        "            # Add entity patterns\n",
        "            self._setup_entity_extractor()\n",
        "\n",
        "            # Prepare response generator data\n",
        "            responses_df = self._prepare_response_data(processed_data)\n",
        "\n",
        "            # Train response generator\n",
        "            self.response_generator.train(responses_df)\n",
        "\n",
        "            # Setup dialogue flow based on dialog acts\n",
        "            self._setup_dailydialog_flow()\n",
        "\n",
        "            print(f\"{self.name} is now trained and ready to chat!\")\n",
        "            return self\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading DailyDialog dataset: {e}\")\n",
        "            print(\"Creating sample data instead...\")\n",
        "            # Fall back to sample data if files not found\n",
        "            return self._train_with_sample_data(test_size)\n",
        "\n",
        "    def _process_dailydialog_data(self, dialogues, topics, acts, emotions):\n",
        "        \"\"\"Process DailyDialog dataset into structured format\"\"\"\n",
        "        utterances = []\n",
        "        topic_labels = []\n",
        "        acts_labels = []\n",
        "        emotion_labels = []\n",
        "        dialogue_ids = []\n",
        "        responses = []\n",
        "\n",
        "        for i, (dialogue, topic, act_seq, emotion_seq) in enumerate(zip(dialogues, topics, acts, emotions)):\n",
        "            # Split the dialogue into utterances\n",
        "            dialogue_utterances = dialogue.strip().split('__eou__')\n",
        "\n",
        "            # Clean up the last element which might be empty\n",
        "            if dialogue_utterances and dialogue_utterances[-1].strip() == '':\n",
        "                dialogue_utterances = dialogue_utterances[:-1]\n",
        "\n",
        "            # Get topic\n",
        "            topic_id = int(topic.strip())\n",
        "            topic_name = self.topic_mapping.get(topic_id, \"Unknown\")\n",
        "\n",
        "            # Get acts and emotions\n",
        "            acts_ids = [int(a) for a in act_seq.strip().split()]\n",
        "            acts_names = [self.act_mapping.get(a, \"Unknown\") for a in acts_ids]\n",
        "\n",
        "            emotion_ids = [int(e) for e in emotion_seq.strip().split()]\n",
        "            emotion_names = [self.emotion_mapping.get(e, \"Unknown\") for e in emotion_ids]\n",
        "\n",
        "            # Check if the lengths match\n",
        "            min_len = min(len(dialogue_utterances), len(acts_ids), len(emotion_ids))\n",
        "\n",
        "            for j in range(min_len):\n",
        "                utterance = dialogue_utterances[j].strip()\n",
        "                if not utterance:\n",
        "                    continue\n",
        "\n",
        "                # Store the utterance data\n",
        "                utterances.append(utterance)\n",
        "                topic_labels.append(topic_name)\n",
        "                acts_labels.append(acts_names[j])\n",
        "                emotion_labels.append(emotion_names[j])\n",
        "                dialogue_ids.append(i)\n",
        "\n",
        "                # For each utterance, find its response (next utterance in dialogue)\n",
        "                if j < min_len - 1:\n",
        "                    response = dialogue_utterances[j+1].strip()\n",
        "                    responses.append(response)\n",
        "                else:\n",
        "                    # If it's the last utterance, use a generic response\n",
        "                    responses.append(\"I understand.\")\n",
        "\n",
        "        return {\n",
        "            'utterances': utterances,\n",
        "            'topic_labels': topic_labels,\n",
        "            'acts_labels': acts_labels,\n",
        "            'emotion_labels': emotion_labels,\n",
        "            'dialogue_ids': dialogue_ids,\n",
        "            'responses': responses\n",
        "        }\n",
        "\n",
        "    def _prepare_response_data(self, processed_data):\n",
        "        \"\"\"Prepare response data for training the response generator\"\"\"\n",
        "        # Create a dataframe with intent (act), template, and response\n",
        "        data = {\n",
        "            'intent': processed_data['acts_labels'],\n",
        "            'intent_template': processed_data['utterances'],\n",
        "            'response': processed_data['responses'],\n",
        "            'emotion': processed_data['emotion_labels'],\n",
        "            'topic': processed_data['topic_labels']\n",
        "        }\n",
        "\n",
        "        # Create DataFrame\n",
        "        responses_df = pd.DataFrame(data)\n",
        "\n",
        "        # Add fallback responses\n",
        "        fallback_responses = [\n",
        "            \"I'm not sure I understand. Could you rephrase your question?\",\n",
        "            \"I didn't catch that. Can you try asking in a different way?\",\n",
        "            \"I'm a bit confused. Could you elaborate?\",\n",
        "            \"I'm still learning. Could you try expressing that differently?\",\n",
        "            f\"I'm {self.name}, and I'm trying to understand. Could you provide more context?\"\n",
        "        ]\n",
        "\n",
        "        for resp in fallback_responses:\n",
        "            new_row = {\n",
        "                'intent': 'fallback',\n",
        "                'intent_template': 'unknown query',\n",
        "                'response': resp,\n",
        "                'emotion': 'neutral',\n",
        "                'topic': 'Unknown'\n",
        "            }\n",
        "            responses_df = pd.concat([responses_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "        return responses_df\n",
        "\n",
        "    def _train_with_sample_data(self, test_size=0.2):\n",
        "        \"\"\"Train with sample data when DailyDialog data is not available\"\"\"\n",
        "        print(\"Training with sample data...\")\n",
        "        intents_df, responses_df = self._create_sample_data()\n",
        "\n",
        "        # Train intent classifier\n",
        "        self.intent_classifier.train(\n",
        "            texts=intents_df['text'].tolist(),\n",
        "            labels=intents_df['intent'].tolist(),\n",
        "            test_size=test_size\n",
        "        )\n",
        "\n",
        "        # Add entity patterns\n",
        "        self._setup_entity_extractor()\n",
        "\n",
        "        # Train response generator\n",
        "        self.response_generator.train(responses_df)\n",
        "\n",
        "        # Setup dialogue flow\n",
        "        self._setup_dialogue_flow()\n",
        "\n",
        "        print(f\"{self.name} is now trained with sample data and ready to chat!\")\n",
        "        return self\n",
        "\n",
        "    def _create_sample_data(self):\n",
        "        \"\"\"Create sample training data\"\"\"\n",
        "        # Sample intents\n",
        "        intents_data = {\n",
        "            'text': [\n",
        "                \"hello\", \"hi there\", \"hey\", \"howdy\",\n",
        "                \"goodbye\", \"bye\", \"see you later\", \"see ya\",\n",
        "                \"what time is it\", \"tell me the time\", \"what's the current time\",\n",
        "                \"what's the weather like\", \"how's the weather today\", \"tell me about the weather\",\n",
        "                \"what can you do\", \"what are your features\", \"help me\", \"what are your capabilities\",\n",
        "                \"tell me a joke\", \"say something funny\", \"do you know any jokes\",\n",
        "                \"who are you\", \"what are you\", \"tell me about yourself\"\n",
        "            ],\n",
        "            'intent': [\n",
        "                \"greeting\", \"greeting\", \"greeting\", \"greeting\",\n",
        "                \"goodbye\", \"goodbye\", \"goodbye\", \"goodbye\",\n",
        "                \"time_query\", \"time_query\", \"time_query\",\n",
        "                \"weather_query\", \"weather_query\", \"weather_query\",\n",
        "                \"help\", \"help\", \"help\", \"help\",\n",
        "                \"joke\", \"joke\", \"joke\",\n",
        "                \"bot_identity\", \"bot_identity\", \"bot_identity\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Sample responses\n",
        "        responses_data = {\n",
        "            'intent': [\n",
        "                \"greeting\", \"greeting\", \"greeting\",\n",
        "                \"goodbye\", \"goodbye\", \"goodbye\",\n",
        "                \"time_query\", \"time_query\",\n",
        "                \"weather_query\", \"weather_query\",\n",
        "                \"help\", \"help\",\n",
        "                \"joke\", \"joke\", \"joke\",\n",
        "                \"bot_identity\", \"bot_identity\",\n",
        "                \"fallback\", \"fallback\"\n",
        "            ],\n",
        "            'intent_template': [\n",
        "                \"hello\", \"hi there\", \"hey\",\n",
        "                \"goodbye\", \"bye\", \"see you later\",\n",
        "                \"what time is it\", \"tell me the time\",\n",
        "                \"what's the weather like\", \"how's the weather today\",\n",
        "                \"what can you do\", \"help me\",\n",
        "                \"tell me a joke\", \"say something funny\", \"do you know any jokes\",\n",
        "                \"who are you\", \"what are you\",\n",
        "                \"unknown query\", \"I don't understand\"\n",
        "            ],\n",
        "            'response': [\n",
        "                \"Hello! How can I help you today?\",\n",
        "                \"Hi there! What can I do for you?\",\n",
        "                \"Hey! What's up?\",\n",
        "                \"Goodbye! Have a great day!\",\n",
        "                \"Bye! Talk to you later!\",\n",
        "                \"See you soon! Take care!\",\n",
        "                \"I'm sorry, I don't have access to the current time.\",\n",
        "                \"I can't tell you the exact time right now.\",\n",
        "                \"I don't have access to weather information currently.\",\n",
        "                \"I can't check the weather for you at the moment.\",\n",
        "                f\"I'm {self.name}, a chatbot that can understand your intents and respond accordingly.\",\n",
        "                \"I can help with basic conversation, answer questions about myself, and more!\",\n",
        "                \"Why don't scientists trust atoms? Because they make up everything!\",\n",
        "                \"What do you call fake spaghetti? An impasta!\",\n",
        "                \"I would tell you a joke about UDP, but you might not get it.\",\n",
        "                f\"I'm {self.name}, an AI assistant built with Python from scratch!\",\n",
        "                \"I'm a custom-built chatbot designed to understand intents and respond naturally.\",\n",
        "                \"I'm not sure I understand. Could you rephrase your question?\",\n",
        "                \"I didn't catch that. Can you try asking in a different way?\"\n",
        "            ],\n",
        "            'emotion': ['neutral'] * 19,  # Add neutral emotion for all sample responses\n",
        "            'topic': ['Ordinary Life'] * 19  # Add default topic for all sample responses\n",
        "        }\n",
        "\n",
        "        # Create DataFrames\n",
        "        intents_df = pd.DataFrame(intents_data)\n",
        "        responses_df = pd.DataFrame(responses_data)\n",
        "\n",
        "        return intents_df, responses_df\n",
        "\n",
        "    def _setup_entity_extractor(self):\n",
        "        \"\"\"Setup entity extraction rules\"\"\"\n",
        "        # Add common patterns for entity extraction\n",
        "        self.entity_extractor.add_entity_pattern(\n",
        "            \"date\",\n",
        "            r\"\\b(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s+\\d{4}\\b|\\b\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}\\b|\\b(?:today|tomorrow|yesterday)\\b\"\n",
        "        )\n",
        "\n",
        "        self.entity_extractor.add_entity_pattern(\n",
        "            \"time\",\n",
        "            r\"\\b(?:\\d{1,2}:\\d{2}(?::\\d{2})?(?:\\s*[ap]\\.?m\\.?)?|\\d{1,2}\\s*[ap]\\.?m\\.?)\\b\"\n",
        "        )\n",
        "\n",
        "        self.entity_extractor.add_entity_pattern(\n",
        "            \"location\",\n",
        "            r\"\\b(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*),?\\s+(?:[A-Z]{2}|[A-Z][a-z]+)\\b\"\n",
        "        )\n",
        "\n",
        "        self.entity_extractor.add_entity_pattern(\n",
        "            \"number\",\n",
        "            r\"\\b\\d+(?:\\.\\d+)?\\b\"\n",
        "        )\n",
        "\n",
        "        self.entity_extractor.add_entity_pattern(\n",
        "            \"email\",\n",
        "            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "        )\n",
        "\n",
        "        # Add patterns for topic-specific entities\n",
        "        self.entity_extractor.add_entity_pattern(\n",
        "            \"health_terms\",\n",
        "            r\"\\b(?:doctor|hospital|clinic|symptoms|treatment|medicine|therapy|disease|illness|patient|diagnosis|healthcare|medical|prescription|allergy|vaccination|surgery)\\b\"\n",
        "        )\n",
        "\n",
        "        self.entity_extractor.add_entity_pattern(\n",
        "            \"finance_terms\",\n",
        "            r\"\\b(?:money|bank|account|loan|credit|debit|investment|stocks|bonds|mortgage|interest|finance|financial|budget|savings|debt|payment|transaction|invoice|currency|dollar|euro|pound|yen|yuan)\\b\"\n",
        "        )\n",
        "\n",
        "    def _setup_dailydialog_flow(self):\n",
        "        \"\"\"Setup dialogue flow based on DailyDialog acts\"\"\"\n",
        "        # Clear existing transitions\n",
        "        self.dialogue_manager.state_transitions = {}\n",
        "\n",
        "        # Initial state\n",
        "        self.dialogue_manager.current_state = \"initial\"\n",
        "\n",
        "        # Add transitions based on dialog acts\n",
        "        self.dialogue_manager.add_state_transition(\"initial\", \"inform\", \"responding_to_inform\")\n",
        "        self.dialogue_manager.add_state_transition(\"initial\", \"question\", \"responding_to_question\")\n",
        "        self.dialogue_manager.add_state_transition(\"initial\", \"directive\", \"responding_to_directive\")\n",
        "        self.dialogue_manager.add_state_transition(\"initial\", \"commissive\", \"responding_to_commissive\")\n",
        "\n",
        "        # From each state, allow transitions to any other state\n",
        "        for source_state in [\"responding_to_inform\", \"responding_to_question\",\n",
        "                             \"responding_to_directive\", \"responding_to_commissive\"]:\n",
        "            for act in [\"inform\", \"question\", \"directive\", \"commissive\"]:\n",
        "                target_state = f\"responding_to_{act}\"\n",
        "                self.dialogue_manager.add_state_transition(source_state, act, target_state)\n",
        "\n",
        "    def _setup_dialogue_flow(self):\n",
        "        \"\"\"Setup dialogue flow for sample data\"\"\"\n",
        "        # Define state transitions\n",
        "        self.dialogue_manager.add_state_transition(\"greeting\", \"greeting\", \"awaiting_query\")\n",
        "        self.dialogue_manager.add_state_transition(\"greeting\", \"help\", \"help\")\n",
        "        self.dialogue_manager.add_state_transition(\"greeting\", \"bot_identity\", \"identity\")\n",
        "\n",
        "        self.dialogue_manager.add_state_transition(\"awaiting_query\", \"time_query\", \"time_response\")\n",
        "        self.dialogue_manager.add_state_transition(\"awaiting_query\", \"weather_query\", \"weather_response\")\n",
        "        self.dialogue_manager.add_state_transition(\"awaiting_query\", \"joke\", \"joke_response\")\n",
        "        self.dialogue_manager.add_state_transition(\"awaiting_query\", \"help\", \"help\")\n",
        "        self.dialogue_manager.add_state_transition(\"awaiting_query\", \"goodbye\", \"farewell\")\n",
        "        self.dialogue_manager.add_state_transition(\"awaiting_query\", \"bot_identity\", \"identity\")\n",
        "\n",
        "        self.dialogue_manager.add_state_transition(\"time_response\", \"greeting\", \"awaiting_query\")\n",
        "        self.dialogue_manager.add_state_transition(\"time_response\", \"goodbye\", \"farewell\")\n",
        "\n",
        "        self.dialogue_manager.add_state_transition(\"weather_response\", \"greeting\", \"awaiting_query\")\n",
        "        self.dialogue_manager.add_state_transition(\"weather_response\", \"goodbye\", \"farewell\")\n",
        "\n",
        "        self.dialogue_manager.add_state_transition(\"joke_response\", \"greeting\", \"awaiting_query\")\n",
        "        self.dialogue_manager.add_state_transition(\"joke_response\", \"goodbye\", \"farewell\")\n",
        "\n",
        "        self.dialogue_manager.add_state_transition(\"help\", \"greeting\", \"awaiting_query\")\n",
        "        self.dialogue_manager.add_state_transition(\"help\", \"goodbye\", \"farewell\")\n",
        "\n",
        "        self.dialogue_manager.add_state_transition(\"identity\", \"greeting\", \"awaiting_query\")\n",
        "        self.dialogue_manager.add_state_transition(\"identity\", \"goodbye\", \"farewell\")\n",
        "\n",
        "        self.dialogue_manager.add_state_transition(\"farewell\", \"greeting\", \"awaiting_query\")\n",
        "\n",
        "    def process(self, user_input):\n",
        "        \"\"\"Process user input and generate response\"\"\"\n",
        "        # Classify intent\n",
        "        intent, confidence = self.intent_classifier.predict(user_input)\n",
        "\n",
        "        # Extract entities\n",
        "        entities = self.entity_extractor.extract_entities(user_input)\n",
        "\n",
        "        # Update dialogue context with entities\n",
        "        self.dialogue_manager.update_context(entities)\n",
        "\n",
        "        # Update dialogue state\n",
        "        current_state = self.dialogue_manager.update_state(intent)\n",
        "\n",
        "        # Generate response\n",
        "        response, response_confidence = self.response_generator.generate_response(\n",
        "            user_input,\n",
        "            intent=intent if confidence > 0.5 else None,\n",
        "            entities=entities\n",
        "        )\n",
        "\n",
        "        # Format response with entity information if applicable\n",
        "        response = self._format_response_with_entities(response, entities)\n",
        "\n",
        "        return {\n",
        "            \"response\": response,\n",
        "            \"intent\": intent,\n",
        "            \"intent_confidence\": confidence,\n",
        "            \"entities\": entities,\n",
        "            \"state\": current_state\n",
        "        }\n",
        "\n",
        "    def _format_response_with_entities(self, response, entities):\n",
        "        \"\"\"Format response with extracted entity information\"\"\"\n",
        "        formatted_response = response\n",
        "\n",
        "        if entities:\n",
        "            entity_info = []\n",
        "            for entity_type, values in entities.items():\n",
        "                if values:\n",
        "                    entity_info.append(f\"{entity_type}: {', '.join(values)}\")\n",
        "\n",
        "            if entity_info:\n",
        "                # Only add entity information if relevant to the response\n",
        "                if \"I detected\" not in formatted_response and \"identified\" not in formatted_response:\n",
        "                    entity_str = \", \".join(entity_info)\n",
        "                    formatted_response += f\"\\n\\nI detected: {entity_str}\"\n",
        "\n",
        "        return formatted_response\n",
        "\n",
        "    def chat(self):\n",
        "        \"\"\"Interactive chat session\"\"\"\n",
        "        print(f\"\\n{self.name}: Hello! How can I help you today? (type 'exit' to quit)\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"You: \").strip()\n",
        "\n",
        "            if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "                print(f\"{self.name}: Goodbye! Have a great day!\")\n",
        "                break\n",
        "\n",
        "            result = self.process(user_input)\n",
        "            print(f\"{self.name}: {result['response']}\")\n",
        "\n",
        "            # Optionally show more details about processing\n",
        "            # print(f\"Debug: Intent: {result['intent']} ({result['intent_confidence']:.2f}), State: {result['state']}\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset chatbot state\"\"\"\n",
        "        self.dialogue_manager.reset()\n",
        "        print(f\"{self.name}: I've reset my conversation memory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NleNTgDGrRwf",
        "outputId": "fa35a22f-b676-4b77-be7b-7f9b43ebfb85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DailyDialog dataset...\n",
            "Training intent classifier on 102979 examples\n",
            "Tokenizing 102979 training examples...\n",
            "Vectorizing text...\n",
            "Training naive_bayes model...\n",
            "Intent Classifier Accuracy: 0.6618\n",
            "DailyBot is now trained and ready to chat!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EnhancedChatbot at 0x7f9121c5de90>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Create and train chatbot with DailyDialog dataset\n",
        "chatbot = EnhancedChatbot(bot_name=\"DailyBot\", model_type=\"naive_bayes\")\n",
        "\n",
        "# Train with DailyDialog data\n",
        "chatbot.train(\n",
        "    dialogues_file=\"dialogues_text.txt\",\n",
        "    topics_file=\"dialogues_topic.txt\",\n",
        "    acts_file=\"dialogues_act.txt\",\n",
        "    emotions_file=\"dialogues_emotion.txt\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9JcxJBbxKLU",
        "outputId": "571d0292-5be9-4eee-f632-0cdce728fa6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DailyBot: Hello! How can I help you today? (type 'exit' to quit)\n",
            "You: Hi\n",
            "DailyBot: Sure.He has already married , a father of two boys .\n",
            "You: exit\n",
            "DailyBot: Goodbye! Have a great day!\n"
          ]
        }
      ],
      "source": [
        "# Start interactive chat\n",
        "chatbot.chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1FJF6Y0NdY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3e851b-2419-4788-e55e-565c82b3ef1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting MWEs. Using empty list.\n",
            "Loading DailyDialog dataset...\n",
            "Training intent classifier on 102979 examples\n",
            "Tokenizing 102979 training examples...\n",
            "Vectorizing text...\n",
            "Training naive_bayes model...\n",
            "Intent Classifier Accuracy: 0.6652\n"
          ]
        }
      ],
      "source": [
        "# Create and train chatbot with DailyDialog dataset\n",
        "chatbot2 = EnhancedChatbot(bot_name=\"DailyBot\", model_type=\"svm\")\n",
        "\n",
        "# Train with DailyDialog data\n",
        "chatbot.train(\n",
        "    dialogues_file=\"dialogues_text.txt\",\n",
        "    topics_file=\"dialogues_topic.txt\",\n",
        "    acts_file=\"dialogues_act.txt\",\n",
        "    emotions_file=\"dialogues_emotion.txt\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79eP7u6q_RpK"
      },
      "outputs": [],
      "source": [
        "chatbot2.chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu_xmrdN_U0E"
      },
      "outputs": [],
      "source": [
        "# Create and train chatbot with DailyDialog dataset\n",
        "chatbot3 = EnhancedChatbot(bot_name=\"DailyBot\", model_type=\"random_forest\")\n",
        "\n",
        "# Train with DailyDialog data\n",
        "chatbot.train(\n",
        "    dialogues_file=\"dialogues_text.txt\",\n",
        "    topics_file=\"dialogues_topic.txt\",\n",
        "    acts_file=\"dialogues_act.txt\",\n",
        "    emotions_file=\"dialogues_emotion.txt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVILmok7_f4_"
      },
      "outputs": [],
      "source": [
        "chatbot3.chat()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSQFpKuzSsOS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}